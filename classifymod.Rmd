---
#title: "5 Machine Learning"
#author: "Kim Cuddington"
date: "05/07/2021"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,kableExtra.auto_format = FALSE)
```

# Classification

Classification is the task of assigning data objects, such as sites, species or images to predetermined classes. Determining what class of data object you have is a question that usually turns on multiple predictors. For example, to classify leaf images to different species, predictors such as size, shape and colour may be used.  If you have satellite data, you may need to classify the different pixels of the image as agricultural, forest, or urban. So for classifcation tasks our response variable, y, is qualitative or categorical (e.g. gender, species, land classification). 

There are many methods that can be employed for classification tasks ranging from logistic regression to random forest techniques. While some of these methods are classic multivariate methods, others, like random forest classifiers, are machine learning tasks. Machine learning is an application of artificial intelligence. The computer algorithm finds a solution to the classifcation problem without being explicitly programmed to do so.

## Logistic regression as a binary classifier

One of the simplest classification methods, and one that does not involve machine learning is logistic regression. Let's take a common example. A non-native species has been introduced to a region, and we would like to know what percentage of the region would be suitable habitat, in order to get an idea of risks of impact on the native ecosystem. We think that average temperature controls habitat suitability, and we have presence/absence data for the species accross a range of different sites. Could we use simple regression to answer the question of whether a given area is suitable habitat? If we indicate absence as 0, and presence as 1, we can regress species occurance again average annual temperatures. 

```{r log, echo=FALSE, fig.height=6, fig.cap="Species presence/absence and mean annual temperature with linear regression", fig.alt=""}
x=rnorm(200,  12,2.5)
y=x
y=ifelse(x>12, 1,0)
e=rnorm(200,0,.1)

y=ifelse(y+e>=1., 1,0)
#y=ifelse(y+e<1., 0,y)

plot(y~x,pch=1,ylim=c(-0.1,1.1), 
     xlim=c(0, 20),col=rgb(0.1, 0.3, 0.4, 0.6),ylab="Species occurance", xlab="Mean annual temperature (°C)", cex.lab=1.5, las=1)
lreg=lm(y~x)
#abline(b=1/20, a=0, col="red")
abline(lreg, col="red", lwd=2)

```

As we can see in Fig. \@ref{fig:log}, the linear regression does not make a lot of sense for a response variable that is restricted to the values of 0 and 1. The regression line $\beta_0+\beta_1x$ can take on any value between negative and positive infinity, but we don't know how to interpret values greater than 1 or less than zero. The regression line almost always predicts wrong value for y in classification problems.

Instead of trying to predict y, we can try to predict p(y = 1), i.e., the probability that the species will be found in the area. For invasive species, this probability if often interpreted as habitat suitability for the species. We need a function that gives outputs between 0 and 1: logistic regression is one solution. In this model, probability of y for a given value of x, p(x) is given as: $$\frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}.$$ Rearranging, we have: $$\frac{p(x)}{1-p(x)}=e^{\beta_0+\beta_1x}.$$ Taking the natural logarithm, we can see that the logistic regression is linear in x: $$\log{\frac{p(x)}{1-p(x)}}=\beta_0+\beta_1x,$$ where the lefthand side is called the log-odds or logit. The logistic function will always produce an S-shaped curve, so regardless of the value of x, we will obtain a sensible prediction.

Let's apply this model to our non-native species data using the *glm()* function. We use the glm() function to perform logistic regression by passing in the family="binomial" argument. But if we use glm() to fit a model without passing in the family argument, then it performs linear regression, just like the lm() function.

```{r, echo=TRUE }
glm.fit.sp=glm(y~x, family=binomial)
glm.probs <- predict(glm.fit.sp,type = "response")
```
```{r logfit, echo=FALSE, fig.height=6, fig.cap="Species presence/absence and mean annual temperature with logistic regression", fig.alt="" }
plot(y~x,pch=16,ylim=c(-0.1,1.1), col=rgb(0.1, 0.3, 0.4, 0.6),ylab="Species occurance", xlab="Mean annual temperature (°C)", cex.lab=1.5, las=1)

logcoefs=round(as.data.frame(summary(glm.fit.sp)$coefficients),2)

curve(exp(logcoefs[1,1]+logcoefs[2,1]*x)/(1+exp(logcoefs[1,1]+logcoefs[2,1]*x)), add=TRUE, col="red", lwd=2)

```

### Interpreting the logistic regression

Let's take a closer look at the output for our logistic regression Fig. \@ref{fig:logfit}. First off, we need to know if the intercept, $\beta_0$, and slope, $\beta_1$, are significantly different from zero.  A z distribution is used for this test, and we find that both the intercept and slope are signficiantly different from zero. \@ref(tab=logcoef). Our p-values are very small, and $\beta_1$ is positive, so we are sure that as temperature increases, the probability of habitat suitability will increase as well.

```{r logcoef}

knitr::kable(logcoefs, digits=2, caption="Logistic regression coefficient estimates and hypothesis tests from species occurance data")
```

The estimated intercept is typically not of interest. Its main purpose is to adjust the average fitted probability to the proportion of ones in the data. You may be confused by the slope value. Interpreting what $\beta_1$ means is not very easy with logistic regression, simply because we are predicting p(y) and not y. If $\beta_1$ = 0, this means there is no relationship between p(y) & x. If $\beta_1$ > 0, this means that when y gets larger so does the probability that y = 1. If $\beta_1$ < 0, this means that when x gets larger, the probability that y = 1 gets smaller. For example, suppose a region has an average annual temperature of 12°C. The probability that the habitat is suitable p(x) for our invasvie species is then: $$p(y)=\frac{e^{paste \beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}.$$
Sustituting in our fitted values from the logisitic regression, we have: 
$$p(y)=\frac{e^{`r logcoefs[1,1]` + `r logcoefs[2,1]` (12)}}{1+e^{`r logcoefs[1,1]` + `r logcoefs[2,1]`(12)}}=`r round((exp(logcoefs[1,1]+logcoefs[2,1]*12))/(1+exp(logcoefs[1,1]+logcoefs[2,1]*12)),2)`.$$ 
The value changes with temperature, so for an annual average temperature of 16°C that probability is `r round((exp(logcoefs[1,1]+logcoefs[2,1]*16))/(1+exp(logcoefs[1,1]+logcoefs[2,1]*16)),2)`, and so on.

Of course, the data used to fit this relationship is zeros (absence) and ones (present). How well does our model do at predicting species occurence? In order to answer this question we need to decide on a *threshold value* for the probability prediction. For example, an easy one is 50%. If the model predicts a probability of greater than 0.5, then we score that as presence, while if the predicted value is equal to or less than 0.5, we'll score as an absence. Now we have model predictions in terms of predicted absences and presences (i.e., zeros and ones), and can compare directly to our data. 

When binary classifications are made, (after converting the probabilities using threshold), there can be four cases for a certain observation:

1. The response actually negative, the model predicts it to be negative. This is known as true negative (TN). In our case, the invasive species is not present at the location, and the model predicts that it should not be present.

2. The response actually negative, but the model predicts it to be positive (i.e., false positive, FP).

3. The response actually positive, and the model predicts it to be positive (i.e., true positive TP).

4. The response actually positive, but the model predicts it to be negative (i.e., false negative FN).

We can summarize this information in a **confusion matrix** which records the number of times the model correctly predicted the data, and the number of times the model makes incorrect predictions. 

```{r, echo=TRUE,comment=""}
#determine model occurance predictions based on threshold value
logocc <- ifelse(glm.probs > 0.5, 1, 0)

#Calculate a confusion matrix
ctab=table(logocc, y)
dimnames(ctab)<-list("Actual"=c("absence(0)","presence(1)"), "Predicted"=c("absence(0)","presence(1)"))
ctab

```

Elements on the diagonal from left to right are correct classifications, while off-diagonal elements are missclassifications (i.e., predictions where the predict code of 0 or 1 does not match the actual code of 0 or 1). So in this case, we have `r ctab[1,1]` true negatives (TN), `r ctab[1,2]` false positives (FP), `r ctab[2,1]` false negatives (FN), and `r ctab[2,2]` true positives (TP).

We can quantify these errors in a number of ways. The misclassification rate, or **error rate** is the most common metric used to quantify the performance of a binary classifier. This is the probability that the classifier makes a wrong prediction (given as $\frac{FN+FN}{TN+FN+TP+FP}$. Overall `r ctab[1,2]+ctab[2,1]` observations have been missclassified, so we have a total error rate of `r round((ctab[1,2]+ctab[2,1])/sum(ctab)*100,0)`%. However, `r round(ctab[2,1]/(ctab[2,1]+ctab[2,2])*100,0)`% of the presence data has been misclassified, compared to `r round(ctab[1,2]/(ctab[1,1]+ctab[1,2])*100,0)`% of the absence data. So with respect to the predicting the potential presence of an invasive species, we're not doing much better than random chance.

The terms sensitivity and specificity characterize the performance of classifier for these specific types of errors. In this case, the 
**sensitivity** is the percentage of occupancy locations that are correctly identified  (true positives), which is `r round(ctab[2,2]/(ctab[2,1]+ctab[2,2])*100,0)`% in this case or one minus the misclassification rate of positives (or 1-`r round(ctab[2,1]/(ctab[2,1]+ctab[2,2]),2)`), or calculated as TP/(TP+FN). **Specificity** is the percentage of non-occupancy sites that are correctly identified (true negatives). We can calcuate this as one minus the misclassification of true negatives from the values above (1 − `r round(ctab[1,2]/(ctab[1,1]+ctab[1,2]),2)`) = `r round(1-ctab[1,2]/(ctab[1,1]+ctab[1,2]),2)`, or from the formula  TN/(TN+FP).

We can of course, change the decision threshold to see if we can get a better outcome. Let's try 0.65 instead of 0.5.
```{r, comment=""}
#determine model predictions based on threshold value
logocc <- ifelse(glm.probs > 0.65, 1, 0)


#Calculate a confusion matrix
ctab=table(logocc, y)
dimnames(ctab)<-list("Actual"=c(0.,1), "Predicted"=c(0,1))
ctab

err=(ctab[1,2]+ctab[2,1])/sum(ctab)
sens=ctab[2,2]/(ctab[2,1]+ctab[2,2])
spec=1-ctab[1,2]/(ctab[1,1]+ctab[1,2])

```
This higher threshold gives us an error rate of `r round(err*100,0)`%, sensitivity of `r round(sens*100,0)`% and specificity of `r round(spec*100,0)`%, so not much improvement. 

We can also examine the performance of the model accross a range of thresholds. We often see this approach in species distribution modelling, where specificity (% of true positives) is plotted again 1-sensitivity (% of false positives) for threshold values from 0 to 1. This graph is called the **R**eceiver **O**perator **C**urve (ROC), and the **A**rea **U**nder that **C**urve (AUC) is calculated. This is a fairly standard evaluation for binary classifiers, and there are a number of R packages that will complete this analysis for you. If the model is not performing better than random chance, the expected ROC curve is simply the y=x line. Where the model can perfectly separate the two classes, the ROC curve consists of a vertical line (x=0) and a horizontal line (y=1). For real and simulated data, usually the ROC stays in between these two extreme scenarios. Let's try with our simulated data.

```{r ROC, echo=TRUE, comment="", fig.caption="Receiver Operator Curve (ROC) for the logistic regression binary classifier of species occurence data", fig.alt=""}
library(ROCit)

ROCit_obj <- rocit(score=glm.fit.sp$fitted.values,class=y)
plot(ROCit_obj)
summary(ROCit_obj)
```
Overall, we have an area under the ROC curve @ref(fig:R) of `r round(ROCit_obj$AUC,2)`, which is not bad, given the maximum value is one. The optimal threshold value is given by the Youden index as `r round(max(ROCit_obj$TPR+(1-ROCit_obj$FPR)-1),2)`. The Youden index  maximizes the difference between sensitivity and 1-specificity and is defined as sensitivity+specificity-1. Let's try this threshold directly.

```{r, comment=""}
#determine model predictions based on threshold value
logocc <- ifelse(glm.probs > round(max(ROCit_obj$TPR+(1-ROCit_obj$FPR)-1),2), 1, 0)

#Calculate a confusion matrix
ctab=table(logocc, y)
dimnames(ctab)<-list("Actual"=c(0.,1), "Predicted"=c(0,1))
ctab

err=(ctab[1,2]+ctab[2,1])/sum(ctab)
sens=ctab[2,2]/(ctab[2,1]+ctab[2,2])
spec=1-ctab[1,2]/(ctab[1,1]+ctab[1,2])

```


We get an error rate of `r round(err*100,0)`%, senstivity of `r round(sens*100,0)`% and specificity of `r round(spec*100,0)`%. Overall, an okay, but not fantastic model, at the best performing threshold.

### Cross-validation

So far, we've evaluated model performance with the data that we used to train the model, but the point of classification tools is to be able to use them on data where we don't already know the answer. In that sense, we are uninterested in in model performance in *training data*, what we really want to do is to test the model on data that was not used in model fitting. For example, we don’t really care how well our method predicts habitat suitability where the invasive species is already located! What we need to know is how well it predicts the habitat suitability of locations where the species has not yet invaded.  The model performance on this *testing data* will give us a better idea of the errors we might expect when we apply our classifer to novel data. While we might naively expect that model performance on the training data will be the same on the testing data, in practice the errors are usually larger, sometimes much larger. In more complex models, this error rate is often the result of overfitting the training data, so that pattern which is just noise is included in the model fit. Consequently, the model is not well fit to data with different sources of noise.

Often course, in biology data is almost always limited! While you might have an extra independent dataset kicking around waiting to be used for model testing, if you don't, you can divide your single dataset into training and testing sets. One easy way to do this is just using random selection. Let's try on our data. We'll divide a dataframe with our data into two parts using the *sample()* function, fit our logistic model on the training data, and evaluate its performance on the testing data.

```{r, echo=TRUE, comment=""}
#simulate some temperature and occupancy data using a random number generator
x=rnorm(200,  12,2.5)
y=ifelse(x>12, 1,0)
e=rnorm(200,0,.1)
#adding some randomness to simulated occupancy data
y=ifelse(y+e>=1., 1,0) 

#create a dataframe with our temperature and occupancy data
ivsp=data.frame(temp=x, occ=y)

#randomly sample 75% of the data (by generating random numbers based on the number of rows)
samp=sample(nrow(ivsp), nrow(ivsp)*.75, replace = FALSE)

#divide into training and testing sets
train <- ivsp[samp, ]
test  <- ivsp[-samp, ]

#fit the logistic model on the training data
log.fit.inv=glm(occ~temp, family=binomial, data=train)

#test the logistic model on the testing data
log.predict <- predict(log.fit.inv,newdata=test,type = "response")

#determine predicted occupancy based on threshold value of 0.5
pred.occ <- ifelse(log.predict >0.5, 1, 0)

#Calculate a confusion matrix
ctab=table(pred.occ, test$occ)
dimnames(ctab)<-list("Actual"=c(0.,1), "Predicted"=c(0,1))
ctab

#Calculate error rate, sensitivity and specificity
err=round((ctab[1,2]+ctab[2,1])/sum(ctab),2)
sens=round(ctab[2,2]/(ctab[2,1]+ctab[2,2]),2)
spec=round(1-ctab[1,2]/(ctab[1,1]+ctab[1,2]),2)

print(paste0("error rate=",err, "; sensitivity=", sens, "; specificity=", spec))

```

You may want to verify for yourself that the sample function randomly selects rows out of a dataframe

You'll notice that by dividing the data up this way we have only 50 observations in our testing data. We can of course use different percentages to divide up our one dataset into training and testing sets, but models tend to have poorer performance when trained on fewer observations. On the other hand, the small testing dataset may tend to overestimate the test error rate for the model fit, as compared to error rates obtained on a larger amount of data. 

And what about the effects of that random sampling? If we repeat the process of randomly splitting the sample set into two parts, we will get a somewhat different estimate for the error rate on testng data each time, because different observations will be randomly included in the dataset each time. Sometimes the differences between error rates on different testing datasets can be rather large. For example, by random selection an observation which is a huge outlier could be included in one small testing dataset, but not in another, resulting is very different error rates. To guard against undue influence of single observations in our small test dataset we could do the routine of randomly sampling to obtain testing and training sets several times, and look at the average of our testing data performance.

#### k-Fold Cross-Validation

One way of implementing this type of resampling scheme is k-fold cross-validation. With this method, we randomly divide the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a testing set, and the model is fit on the remaining k − 1 folds. This procedure is repeated k times, and each time, a different group of observations is treated as the testing set. We then average the error rates from each test fold. We can even repeat the entire procedure several times in *repeated k-fold cross-validation*.

```{r, echo=TRUE, eval=FALSE}
#example code for iterated cross-validation set up
reps=10
nfolds=5

for (j in 1:reps){

# generate array containing fold-number for each sample (row)
foldsset <- rep_len(1:nfolds, nrow(data))
folds <- sample(foldsset, nrow(data))

# actual cross validation
for(k in 1:nfolds) {
  
    # split of the data
    fold <- which(folds == k)
    data.train <- data[-fold,]
    data.test <- data[fold,]

    # train and test your model with data.train and data.test
}
}

```


### Multiple logistic regression
If we have more than one predictor, we can fit multiple logistic just like regular regression, as:
$$p(y)=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}$$
and the $x_n$ predictors can be both qualitiative or quantiative. For example we could add a land classification to our invasive species habitat suitability model so that we have both temperature ($x_1$) and urban and rural ($x_2$) land types.

```{r}
x2=ifelse(y==1, "urban", "rural")
e2=rnorm(200,0,.1)
x2=ifelse(y+e2<1., "rural",x2 )
x2=ifelse(e2>0.1, "urban", x2)
```



```{r, echo=TRUE, comments=""}
glm.fit.sp2=glm(y~x+as.factor(x2), family=binomial)
```


In this case, our model now has two responses, one for land categorized as urban, and one for land categorized as rural (Fig @ref(fig:multilog)).




```{r logcoefs2}
logcoefs2=round(as.data.frame(summary(glm.fit.sp2)$coefficients),2)
knitr::kable(logcoefs2, digits=2, caption="Logistic regression coefficient estimates and hypothesis tests from species occurance data with temperature and land category as predictors")
```

```{r multilog, fig.cap="Species occurance vs temperature and land classification as urban or rural", fig.alt=""}
plot(y~x, col=as.factor(x2),ylab="Species occurance", xlab="Mean annual temperature (°C)", cex.lab=1.5, las=1)
legend("topleft", c("urban", "rural"), pch=1, col=c("red", "black"), bty="n")


curve(exp(logcoefs2[1,1]+logcoefs2[2,1]*x)/(1+exp(logcoefs2[1,1]+logcoefs2[2,1]*x)), add=TRUE, col="black", lwd=2)

curve(exp(logcoefs2[1,1]+logcoefs2[2,1]*x+logcoefs2[3,1])/(1+exp(logcoefs2[1,1]+logcoefs2[2,1]*x+logcoefs2[3,1])), add=TRUE, col="red", lwd=2)

```

both predictors are significantly different from zero (Table @ref(tab:logcoefs), and the values tell is that if the land is categorized as urban, we must increase our probability estimate upwards from that of a rural area as:  
$$p(y)=\frac{e^{`r logcoefs2[1,1]` + `r logcoefs2[2,1]` x_1+`r logcoefs2[3,1]` (1)}}{1+e^{`r logcoefs2[1,1]` + `r logcoefs2[2,1]`x_1+`r logcoefs2[3,1]` (1)}}$$

Logistic regression can be extended to multiple classification problems in different ways, but in practice these methods tend not to be used all that often. Instead other technqiues such as discriminant analysis and random forest tend to be used for multiple-class classification problems.

### Exercise: Logistic regression as a binary classifier

Try to use logistic regression on your own, with a pre-existing dataset in the MASS package. Start by installing the MASS package, load the library and load the data. We'll be using the Pima.tr data in the MASS package, which describes risk factors for diabetes. Type help(Pima.tr) or ?Pima.tr to get a description of these data. You’ll notice that the “type” variable is our classifier and determines whether the patient has diabetes or not.


```{r pressure, echo=TRUE, comment=""}
library(MASS)
library(caret)
data(Pima.tr)
str(Pima.tr)
```


Next construct a logistic regression, to use as a classifier.  Examine your output to determine if the regression is significant. We can use the . in the formula argument to mean that we use all the remaining variables in the dataset as predictors. 

```{r, echo=TRUE, comment=""}
#run logistic regression
Pima.log<-glm(type~., family=binomial,data=Pima.tr)
summary(Pima.log)

```

Not all the predictors have slopes significantly different from zero, but certainly glucose looks important. If you wanted you could simplify this model, and compare the different models with AIC, just like a standard linear regression.

Next, we’ll use some testing data, to test our classifier. The Prima.te has already been created for you in the MASS package. Use the predict function to get the predicted probabilities, and a threshold value to get classifications.  Then construct a confusion matrix to determine how well our predictor did.

```{r, echo=TRUE}
testPima<-predict(Pima.log, newdata=Pima.te, type="response")
testPima=as.factor(ifelse(testPima>0.5, "Yes", "No"))

```


```{r, comment=""}

#Calculate a confusion matrix
atab=table(Pima.te$type, testPima)
dimnames(atab)<-list("Actual"=levels(Pima.te$type), "Predicted"=levels(Pima.te$type))

atab
```


Notice there there have been some missclassifications at 50%, and that the accuracy is only about `r round(sum(diag(atab))/sum(atab),2)`. See if another decision boundary (e.g., 75%) does any better.

There is a nice package in R, caret, that is a good wrapper for these kinds of tasks, and which is really great for machine learning. We can use it to generate both our confusion matrix, and other statistical info about our classifier

```{r, echo=TRUE,comment=""}
library(caret)
confusionMatrix(Pima.te$type, testPima)
```

Perhaps the most informative of these stats is the *No Information Rate* which tests whether our classifier does better than random assignment. We see that our error rate is signficantly greater than this no information rate. Also, the *Balanced Accuracy Statistic* gives an accuracy value that weights both majority and minority classes evenly, if you have unbalanced class membership in your data.


## Tree-based methods for classification
Tree-based methods for classification involve dividing up regions defined by the predictor variables. For example, simple species identfication trees use this approach. You are looking at a tree: does it have needle- or scale-shaped leaves? Or are the leaves wide? The leaf characteristic predictor is used to divide up the classification possibilities into conifers and deciduous trees (not without errors!). So we repeatedly split the response data into two groups that are as homogeneous as possible. The split is determined by the single predictor that best discriminates among the data. The binary splits continue to partition the data into smaller and smaller groups, or nodes, until the groups are no longer homogeneous. This effort produces a single tree where the binary splits form the branches and the final groups compose the terminal nodes, or leaves.

If this type of method is applied with a continuous response variable instead it is called a *regression tree*, if the response is categorical, it is called a *classification tree*. Often we refer to both technqiues at the same time as classification and regression trees (CART). When used as a regression response predictor, this technqiue differs from standard regression approaches which are global models where the predictive formula is supposed to hold in the entire data space. Instead trees try to partition the data space into small enough parts where we can apply a simple different model on each part. 

Let’s try a simple example on the iris data. We’ll first split the data into a training and testing set, and then run our classfication tree algorithm in the rpart package.

```{r firstree, fig.width=10, fig.height=7., fig.cap="A simple plot of a tree classifier for the iris data", fig.alt="",echo=TRUE}
library(rpart)
alpha     <- 0.7 # percentage of training set
inTrain   <- sample(1:nrow(iris), alpha * nrow(iris))
train.set <- iris[inTrain,]
test.set  <- iris[-inTrain,]

mytree <- rpart(Species ~ ., data = train.set, method = "class")
par(mar=c(5,3,3,3))
plot(mytree)
text(mytree)
```

We can see that petal length is used to distinguish species *I.setosa* from the other two species, and then petal width classifies into *I. versicolor* or *I. virginia*. The model of course includes more information than this regarding the number of observations aggregating to each branch of the tree etc. More detailed information can be obtained from *summary(mytree)*, or just typing mytree. A nicer plot, with more details can also be obtained with rpart.plot library.

```{r fancytree, fig.cap="A nicer plot of a tree classifier for the iris data made with the rpart.plot package", fig.alt="",echo=TRUE}
library(rpart.plot)
rpart.plot(mytree)
```

This plot (Fig. @ref(fig:fancytree)), in addition to the factor that splits each branch, also tells us the percentage of the data in each class, and the percentage that travels down each branch in each class. Starting at the top, each species makes up roughly a third of the data, after the petal length branch, travelling down the petal length greater than or equal to 2.5, all I.setosa observations all on the other side of the split, and we are left with data divided evenly between the *I.versicolor* and *I. viriginica*. The petal length < 4.8 branch separates out these two species, with some error in classification.

The algorithm determines which variable to split based on **impurity**, or how similar points are within a group. If all data points are identical, then impurity is zero. Impurity increases as points become more dissimilar. Impurity is calculated differently for different kinds of trees. For classification trees: the Gini index, which reflects rhe proportion of responses in each level of a categorical variable is often used. The Gini index is calculated as: $Gini=1-\sum p_i$, where $p_i$ is the proportion of observations in each class. The Gini index is small when many observations fall into a single category, so the split is made at the single variable which minimizes the Gini index. Some classifiers use the Shannon-Weiner index instead, which has similar properties.

Using this tree classifer, we can make predictions for our testing data, and get a confusion matrix

```{r, echo=TRUE, comment=""}
pred <- predict(mytree, test.set, type="class")
confusionMatrix(pred, test.set$Species)
```
Our accuracy is pretty good for all species.

### Tree pruning

So how does the model decide when to stop? Presumably you could continue to build out the tree until every single observation is a node. Another way to phrase this question is: how do you prevent the model from overfitting the data? The answer is: pruning (the best part about this classification method is the metaphorical gardening language!). Pruning is the act of overgrowing the tree and then cutting it back. Ultimately pruning should yield a tree that optimizes the trade-off between complexity and predictive ability.

Pruning begins by creating a nested series of trees of increasing number of branches, from 0 (no splits) to however many can be reasonably obtained from the data. For each number of branches, an optimal tree can be recovered, i.e., one that minimizes the overall misclassification rate.
To select the tree of optimal size we use cross-validation. For a given tree size, cross-validation divides the data into equal portions, removes one portion from the data, builds a tree using the remaining portion, and then calculates the error between the observed data and the predictions. 
This procedure is repeated for each of the remaining portions and then the overall error is summed across all subsets of the data. 
This is done for each of the nested trees. The tree of optimal size is then determined based on the smallest tree that is with in 1 standard error of the minimum error observed across all trees.

Even with pruning, a single CART is likely to overfit the data, particularly when there are many, many predictors, and thus is not very good for prediction. One way to get around this is to build a bunch of different, non-nested, trees on subsets of the data, and then average accross them. Because any given tree is constructed with only a portion of the data, the likelihood of overfitting is drastically reduced. Moreover, averaging across many trees reduces the impact of anomalous results from a single tree. This is the idea of *ensemble learning*, or combining many ‘weak learners’ (individual trees) to produce one ‘strong learner’ (the ensemble).

### Random Forests

One type of ensemble decision tree is a random forest. Random forests use a bootstrapped sample of the data, and only a portion of the predictors to construct each tree. This procedure ensures that each individual tree is independent from the others, making it a much more accurate method than some other ensemble learning techniques (e.g., bagging). As well, since both the data and the predictors are subsampled, these models can be fit to more predictors than there are observations. This seems a little counterintuitive, but can be a real benefit for ecological data which typically suffers from low replication.

Let’s try an ensemble decision tree on the iris data. We use the randomForest package on the training data. Note that we do not have to split our data into training and testing now, the random Forest package is already doing this sort of thing for us.

```{r, echo=TRUE, comment="", message=FALSE}
library(randomForest)
RF.model = randomForest(Species ~ ., data = iris)
RF.model

```

Our classification is pretty good with a misclassification rate  on the **O**ut **O**f **B**ag data of only 4% (remember this is the equivalent to the test data from a cross-validation), and errors for invidual species from 0 to 0.06%. We might like to look at what the model is doing but unlike a single CART, random forests do not produce a single visual, since of course the predictions are averaged across many hundreds or thousands of trees. 


When building random forests, there are three tuning parameters of interest: node size, number of trees, and number of predictors sampled at each split. Careful tuning of these parameters can prevent extended computations with little gain in error reduction. For example, the plot below @ref(fig:RFOOB) shows how the overall OOB error rate, and the error rate for each of the three species, changes with the size of the forest (the number of trees).

```{r RFOOB, fig.caption="Out of bag error, and individual classification errors for the three species classes in the random forest model", fig.alt=""}
plot(RF.model, main="")
legend("topright", c("OOB", "setosa", "versicolor", "virginica"), col=c(1,2,3,4), lty=c(1,2,3,4), bty="n")

```


Obviously with few trees the error rate is high, but as more trees are added you can see the error rate decrease and eventually flatten out. in the above plot, we could easily reduce the number of trees down to 300 and experience relatively little loss in predictive ability. This is easy to do:

```{r, echo=TRUE, comment=""}
update(RF.model, ntree = 300)
```

So we see an increase in our error rate, but not much.

Despite not yielding a single visualizable tree, one of the major advantages of random forests is that they can provide a measure of relative importance. By ranking predictors based on how much they influence the response, random forests may be a useful tool for selecting predictors before trying another framework, such as CART. Importance can be obtained using the importance function, and plotted using the varImpPlotfunction:

```{r VIP, echo=TRUE, fig.caption="Variable importance plot for our random forest model of the iris data", fig.alt=""}
varImpPlot(RF.model) 
```


Figure @ref(fig:VIP) reports the mean decrease in the Gini Index for each predictor. If you recall, the Gini index is a measure of impurity for categorical data. For each tree, each predictor in the OOB sample is randomly permuted (aka, shuffled around) and passed to the tree to obtain the error rate. The error rate from the unpermuted OOB is then subtracted from the error rate on the permuted OOB data, and averaged across all trees. When this value is large, it implies that a variable had a strong relationship with the response. That is, the model got much worse at predicting the data when that variable was permuted. As we already knew, Petal.Length and Petal.Width are the two most important variables. 

One other useful aspect of random forests is getting a sense of the partial effect of each predictor given the other predictors in the model. This has analogues to partial correlation plots in linear models. We can construct a partial effecs response by holding each value of the predictor of interest constant (while allowing all other predictors to vary at their original values),passing it through the random forest, and predicting the responses. The average of the predicted responses are plotted against each value of the predictor of interest (the ones that were held constant) to see how the effect of that predictor changes based on its value. This exercise can be repeated for all other predictors to gain a sense of their partial effects.

The function to calculate partial effects is partialPlot. Let’s look at the effect of Petal.Length:
```{r PEplot, echo=TRUE, fig.cap="Partial effect of petal length in the random forest model of the iris data", fig.alt=""}
partialPlot(RF.model, iris, "Petal.Length", main="", ylab="log odds") 
```


The y-axis is a bit tricky to interpret (Fig. @ref(fig:PEplot). Since we are dealing with classification trees, y on the logit scale, and is the probability of success. In this case, the partial plot has defaulted to the first class, which is *I. setosa*. The plot says that there is a high chance of successfully predicting this species from Petal.Length when Petal.Length is less than around 2.5 cm, after which point the chance of successful prediction drops off precipitously. This is actually quite reassuring as this is the first split identified way back in the very first CART (where the split was < 2.45 cm).

**Missing data**

Its worth noting that the default behavior of randomForest is to refuse to fit trees with missing predictors. You can, however, specify a few alternative arguments: the first is na.action = na.omit, which removes the rows with missing values outright. Another option is to use na.action = na.roughfix, which replaces missing values with the median (for continuous variables) or the most frequent level (for categorical variables). Missing responses are harder: you can either remove that row, or use the function rfImpute to impute values. The imputed values are the average of the non-missing observations, weighted by their proximity to non-missing observations (based on how often they fall in terminal nodes with those observations). rfImpute tends to give optimistic estimates of the OOB error.

**What else?**

We've just provided a small sampler of classification methods here that will get you started on your way. But other methods such as Linear Discriminent Analysis (LDA(, Artificial Neural Network (ANN), K-nearest neighbors, and Support Vector Machines (SVM) are also useful for building classification models.

